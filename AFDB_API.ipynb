{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulynamagana/AFDB_notebooks/blob/main/AFDB_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Accessing AlphaFold DB structures via API**\n",
        "\n",
        "\n",
        "This Colab notebook allows you to search for and download protein structures from the AlphaFold database using the AlphaFold API. You can also filter entries based on pLDDT score.\n",
        "\n",
        "To use this Colab notebook, you will need to have a Google account and be logged in to Google Colab.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  ## How to use Google Colab <a name=\"Quick Start\"></a>\n",
        "1. To run a code cell, click on the cell to select it. You will notice a play button (▶️) on the left side of the cell. Click on the play button or press Shift+Enter to run the code in the selected cell.\n",
        "2. The code will start executing, and you will see the output, if any, displayed below the code cell.\n",
        "3. Move to the next code cell and repeat steps 2 and 3 until you have executed all the desired code cells in sequence.\n",
        "4. The currently running step is indicated by a circle with a stop sign next to it.\n",
        "If you need to stop or interrupt the execution of a code cell, you can click on the stop button (■) located next to the play button.\n",
        "\n",
        "*Remember to run the code cells in the correct order, as their execution might depend on variables or functions defined in previous cells. You can modify the code in a code cell and re-run it to see updated results.*\n",
        "\n",
        "## Contact us\n",
        "\n",
        "If you experience any bugs please contact afdbhelp@ebi.ac.uk\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4U16tkimZcaG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZTI_3yW0rTC",
        "outputId": "f9a21491-eb81-44eb-a6a2-ba471e73e9f1",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using provided taxonomy ID: 574521wdqwed\n"
          ]
        }
      ],
      "source": [
        "#@title ##1.&nbsp; Input Organism name or Taxonomy ID\n",
        "import requests, sys, json\n",
        "import re\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import concurrent.futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "\n",
        "\n",
        "#@markdown You need to provide Organism name\n",
        "Organism = \"\" #@param {type:\"string\"}\n",
        "#### IF TAXONOMY ID WAS PROVIDED THEN SAVE IT WAS TAX ID\n",
        "#@markdown OR Taxonomy ID\n",
        "tax_ID = \"574521wdqwed\" #@param {type:\"string\"}\n",
        "\n",
        "Protein_status = \"Reviewed\" #@param [\"Reviewed\", \"Unreviewed\", \"All\"]\n",
        "#@markdown - `Reviewed` will query only Reviewed entries `Unreviewed` will query only Unreviewed proteins\n",
        "\n",
        "pLDDT_filter = \">95\" #@param {type:\"string\"}\n",
        "pLDDT_filter = pLDDT_filter.strip()\n",
        "\n",
        "# Extract the comparison operator and value from user input\n",
        "operator, value = pLDDT_filter[:1], int(pLDDT_filter[1:])\n",
        "\n",
        "# Build the condition string dynamically\n",
        "condition = f\"confidence_score {operator} {value}\"\n",
        "\n",
        "#@markdown - This is the minimum average pLDDT to filter the structures from th ORganism or tax_ID\n",
        "\n",
        "folder_save = \"test\" #@param {type:\"string\"}\n",
        "#@markdown - This will create a folder inside a folder names AFDB_API_files\n",
        "\n",
        "\n",
        "if len(Organism.split()) == 2:\n",
        "  word1, word2 = Organism.split()\n",
        "  formatted_Organism = f\"{word1}%20{word2}\"\n",
        "\n",
        "else:\n",
        "  formatted_Organism = Organism\n",
        "\n",
        "######## ADD AN IF, IF THE ORGANISM IF NOT EMPTY, THEN RUN THE CODE\n",
        "\n",
        "# Define the base URL\n",
        "BASE_URL = \"https://www.ebi.ac.uk/proteins/api/proteomes?offset=0&size=100&name=\"\n",
        "# Construct the full URL\n",
        "WEBSITE_API = BASE_URL + formatted_Organism\n",
        "\n",
        "\n",
        "\n",
        "# Check if at least one variable is provided\n",
        "if Organism and tax_ID:\n",
        "    print(\"Error, please provide either organism or taxonomy ID, not both.\")\n",
        "elif Organism:\n",
        "# Make a GET request to the URL\n",
        "  response = requests.get(WEBSITE_API)\n",
        "# Check if the request was successful (status code 200)\n",
        "  if response.status_code != 200:\n",
        "    # Parse the JSON response\n",
        "      print(json.dumps(response.json(), indent=3))\n",
        "# Access the the JSON data\n",
        "  response_data = response.json()\n",
        "  data= list(response_data)[0]\n",
        "  #extract the value from the key taxonomy\n",
        "  if 'taxonomy' in data:\n",
        "      tax_ID = data['taxonomy']\n",
        "      print(f\"Using taxonomy ID: {tax_ID} for Organism: {Organism}\")\n",
        "  else:\n",
        "      print(\"taxonomy ID not found in the JSON data.\") #if not found in the JSON\n",
        "elif tax_ID: # Use the provided taxid\n",
        "    print(f\"Using provided taxonomy ID: {tax_ID}\")\n",
        "else:\n",
        "    print(\"Error, you need to provide organism or taxonomy ID.\") #user didn't provide organism or tax_ID\n",
        "\n",
        "\n",
        "#####\n",
        "if Protein_status == \"Reviewed\":\n",
        "  status = \"+AND+%28reviewed%3Atrue%29\"\n",
        "elif Protein_status == \"Unreviewed\":\n",
        "  status = \"+AND+%28reviewed%3Afalse%29\"\n",
        "else:\n",
        "  status = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##2.&nbsp; Entries to Scan on UniProt\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import re\n",
        "\n",
        "BASE_URL_API = \"https://rest.uniprot.org/uniprotkb/search?fields=accession%2Cid%2Cprotein_name%2Cgene_names&format=tsv&query=%28%28taxonomy_id%3A+\"\n",
        "QUERY_UniprotIDs = f\"{BASE_URL_API}{tax_ID}%29%29{status}&size=500\"\n",
        "\n",
        "re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
        "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
        "session = requests.Session()\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
        "\n",
        "def get_next_link(headers):\n",
        "    if \"Link\" in headers:\n",
        "        match = re_next_link.match(headers[\"Link\"])\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "def get_batch(batch_url):\n",
        "    total = 0  # Initialize total outside of the loop\n",
        "    printed_total = False  # Flag to track whether the total has been printed\n",
        "    while batch_url:\n",
        "        try:\n",
        "            response = session.get(batch_url)\n",
        "            response.raise_for_status()\n",
        "            total = response.headers[\"x-total-results\"]\n",
        "\n",
        "            # Print total only once before the loop\n",
        "            if not printed_total:\n",
        "                print(f\"Total items to retrieve: {total} for Organism `{Organism}` and Protein status `{Protein_status}`\")\n",
        "                printed_total = True\n",
        "\n",
        "                # Ask for user confirmation\n",
        "                user_input = input(\"Do you want to proceed with the download? (y/n): \").strip().lower()\n",
        "                if user_input != 'y':\n",
        "                    print(\"Download aborted.\")\n",
        "                    return\n",
        "\n",
        "            yield response, total\n",
        "            batch_url = get_next_link(response.headers)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nInterrupted by user.\")\n",
        "            return\n",
        "\n",
        "primary_accessions = set()  # Use a set to ensure unique primaryAccessions\n",
        "\n",
        "try:\n",
        "    for batch, total in get_batch(QUERY_UniprotIDs):\n",
        "        for line in batch.text.splitlines()[1:]:\n",
        "            primary_accession = line.split('\\t')[0]\n",
        "            primary_accessions.add(primary_accession)\n",
        "\n",
        "        print(f'{len(primary_accessions)} / {total}')\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nInterrupted by user.\")\n",
        "\n",
        "# Convert set to list\n",
        "primary_accessions_list = list(primary_accessions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "8xaAC4LAWl4S",
        "outputId": "cff5a05e-4126-4ffb-9d28-5b817b93aafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total items to retrieve: 779 for Organism `` and Protein status `Reviewed`\n",
            "Do you want to proceed with the download? (y/n): y\n",
            "500 / 779\n",
            "779 / 779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##3.&nbsp; Filter pLDDT entries\n",
        "import requests\n",
        "import concurrent.futures\n",
        "from requests.adapters import HTTPAdapter\n",
        "from requests.packages.urllib3.util.retry import Retry\n",
        "import time\n",
        "\n",
        "#API URL to retrieve the summary\n",
        "api_url = \"https://alphafold.ebi.ac.uk/api/uniprot/summary/\"\n",
        "\n",
        "def get_data(accession):\n",
        "    url = f\"{api_url}{accession}.json\" #Construct the URl for API\n",
        "\n",
        "    # Configure a retry mechanism with exponential backoff\n",
        "    retries = Retry(total=3, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
        "    adapter = HTTPAdapter(max_retries=retries)\n",
        "\n",
        "    # Make a request using a session with the configured adapter\n",
        "    with requests.Session() as session:\n",
        "        session.mount('https://', adapter)\n",
        "        response = session.get(url)\n",
        "\n",
        "    #Check if the request was successfull\n",
        "    if response.status_code == 200:\n",
        "        data = response.json() #Parse the JSON response\n",
        "\n",
        "        # Check confidence score first\n",
        "        structures = data.get(\"structures\", [])\n",
        "        first_structure = structures[0] if structures else {}\n",
        "\n",
        "        summary = first_structure.get(\"summary\", {})\n",
        "        confidence_score = summary.get(\"confidence_avg_local_score\", 0)\n",
        "\n",
        "        # Evaluate the confidence condition and extract data if met\n",
        "        if eval(condition):\n",
        "            # Confidence score is sufficient, extract and return \"ac\"\n",
        "            uniprot_entry = data.get(\"uniprot_entry\", {})\n",
        "            ac_value = uniprot_entry.get(\"ac\")\n",
        "            return ac_value\n",
        "\n",
        "    # Confidence score not met or an issue with the API request\n",
        "    return None\n",
        "\n",
        "# Use ThreadPoolExecutor for concurrent API calls within a chunk\n",
        "def get_data_chunk(chunk):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = list(executor.map(get_data, chunk))\n",
        "    return [result for result in results if result is not None]\n",
        "\n",
        "#Specify chunk size and sleep interval\n",
        "chunk_size = 40\n",
        "sleep_interval = 1.5\n",
        "\n",
        "#Split the accessions into chunks\n",
        "accession_chunks = [primary_accessions_list[i:i + chunk_size] for i in range(0, len(primary_accessions_list), chunk_size)]\n",
        "\n",
        "#Start result storage and track total chunks\n",
        "all_results = []\n",
        "total_chunks = len(accession_chunks)\n",
        "\n",
        "#Start processed chunk count\n",
        "processed_chunks = 0\n",
        "\n",
        "#Iterate through chunks\n",
        "for chunk in accession_chunks:\n",
        "    retry_count = 0\n",
        "    while retry_count < 3:  # Maximum retries\n",
        "        try:\n",
        "          #Get and store data for the current chunk\n",
        "            results = get_data_chunk(chunk)\n",
        "            all_results.extend(results)\n",
        "            break\n",
        "        except requests.RequestException as e:\n",
        "            #print(f\"Error: {e}\") # for testing errors\n",
        "            retry_count += 1 # Handle API request, introduce a delay\n",
        "            time.sleep(sleep_interval)\n",
        "\n",
        "    #Print progress after processing each chunk\n",
        "    processed_chunks += 1\n",
        "    print(f\"Processed {processed_chunks} / {total_chunks} chunks\")\n",
        "\n",
        "    #Delay between chunks\n",
        "    time.sleep(sleep_interval)\n",
        "\n",
        "# Print final progress\n",
        "print(f\"Processed {processed_chunks} / {total_chunks} chunks\")\n",
        "#print(all_results) #test entries after pLDDT filter\n",
        "\n",
        "\n",
        "print(f\"Total entries after filtering by pLDDT: {len(all_results)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "bApF1PTmhC4L",
        "outputId": "66e617e9-4353-4224-a320-afb2fc0ba185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1 / 20 chunks\n",
            "Processed 2 / 20 chunks\n",
            "Processed 3 / 20 chunks\n",
            "Processed 4 / 20 chunks\n",
            "Processed 5 / 20 chunks\n",
            "Processed 6 / 20 chunks\n",
            "Processed 7 / 20 chunks\n",
            "Processed 8 / 20 chunks\n",
            "Processed 9 / 20 chunks\n",
            "Processed 10 / 20 chunks\n",
            "Processed 11 / 20 chunks\n",
            "Processed 12 / 20 chunks\n",
            "Processed 13 / 20 chunks\n",
            "Processed 14 / 20 chunks\n",
            "Processed 15 / 20 chunks\n",
            "Processed 16 / 20 chunks\n",
            "Processed 17 / 20 chunks\n",
            "Processed 18 / 20 chunks\n",
            "Processed 19 / 20 chunks\n",
            "Processed 20 / 20 chunks\n",
            "Processed 20 / 20 chunks\n",
            "Total entries after filtering by pLDDT: 306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##4.&nbsp; Download files to Google Drive\n",
        "#@markdown **Note:** This will download all the mmCIF files and PAE for the accessions with specified filter for average pLDDT <br>\n",
        "#@markdown You will find a all files in folder \"AFDB_API_files\" and a file called summary.csv\n",
        "import os\n",
        "import csv\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "destination_path = f\"/content/drive/MyDrive/AFDB_API_files/{folder_save}\"\n",
        "\n",
        "isExist = os.path.exists(destination_path)\n",
        "if not isExist:\n",
        "    os.makedirs(destination_path)\n",
        "    print(\"The new directory was created!\")\n",
        "\n",
        "# Function to download a file from a given URL and save it to the Google Drive\n",
        "def download_file(url):\n",
        "    os.chdir(destination_path)\n",
        "    !wget \"$url\" -q\n",
        "\n",
        "# List to store data for CSV\n",
        "data_list = []\n",
        "\n",
        "BASE_URL = \"https://alphafold.ebi.ac.uk/api/prediction/\"\n",
        "\n",
        "# Function to process an entry\n",
        "def process_entry(entry):\n",
        "    api_url = BASE_URL + entry\n",
        "    response = requests.get(api_url)  # Make a GET request to the URL\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the JSON response\n",
        "        api_data_list = response.json()\n",
        "\n",
        "        # Assuming there is only one dictionary in the list\n",
        "        api_data = api_data_list[0]\n",
        "\n",
        "        # Extract relevant information from the API response\n",
        "        uniprot_description = api_data.get(\"uniprotDescription\", \"\")\n",
        "        uniprot_accession = api_data.get(\"uniprotAccession\", \"\")\n",
        "        organism_scientific_name = api_data.get(\"organismScientificName\", \"\")\n",
        "        model_created_date = api_data.get(\"modelCreatedDate\", \"\")\n",
        "        reviewed = api_data.get(\"isReviewed\", \"\")\n",
        "\n",
        "        ##download\n",
        "        download_file(api_data.get(\"cifUrl\", \"\"))\n",
        "        download_file(api_data.get(\"paeDocUrl\", \"\"))\n",
        "\n",
        "        # Append data to the list\n",
        "        data_list.append([uniprot_description, uniprot_accession, organism_scientific_name, model_created_date, reviewed])\n",
        "\n",
        "# Specify the number of threads\n",
        "num_threads = 4\n",
        "\n",
        "# Use ThreadPoolExecutor to parallelize the downloading process\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    executor.map(process_entry, all_results)\n",
        "\n",
        "# Specify the CSV file path\n",
        "csv_file_path = '/content/drive/MyDrive/AFDB_API_files/summary.csv'\n",
        "\n",
        "# Writing data to CSV\n",
        "with open(csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write header\n",
        "    writer.writerow(['Uniprot Description', 'Uniprot Accession', 'Organism Scientific Name', 'Model Created Date', 'Reviewed'])\n",
        "\n",
        "    # Write data\n",
        "    writer.writerows(data_list)\n",
        "\n",
        "print(f\"Data has been downloaded to {csv_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cieR33MytZT4",
        "outputId": "c1383db1-2ca0-4733-b90b-7ffaf3f3324d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data has been downloaded to /content/drive/MyDrive/AFDB_API_files/summary.csv\n"
          ]
        }
      ]
    }
  ]
}